{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from board_processor import BoardProcessor\n",
    "from feature_generator import FeatureGenerator\n",
    "import os\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=138):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for h in [256, 128, 64, 32, 16, 8]:\n",
    "            layers.extend([nn.Linear(input_dim, h), nn.Tanh()])\n",
    "            input_dim = h\n",
    "        layers.extend([nn.Linear(h, 1), nn.Tanh()])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)"
   ],
   "id": "8d39ada4cd0686eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def get_current_q(state_action_features, online_model, scaler, player):\n",
    "    \"\"\"Get current Q-value prediction from online model\"\"\"\n",
    "    scaled = scaler.transform([state_action_features])\n",
    "    with torch.no_grad():\n",
    "        q = online_model(torch.FloatTensor(scaled).to(online_model.device if hasattr(online_model, 'device') else 'cpu')).item()\n",
    "    return q * player  # Adjust for player perspective\n",
    "\n",
    "def calculate_target_q(moves, position_i, player, online_model, target_model, scaler, feature_gen, gamma=0.99):\n",
    "    \"\"\"Calculate target Q-value using Double DQN logic\"\"\"\n",
    "    # Check if game ends after our move (position i+1)\n",
    "    board_after_our_move = BoardProcessor()\n",
    "    board_after_our_move.generate_state_list(moves[:position_i+1])\n",
    "    _, feats_after_our_move = feature_gen.convolution_feature_gen(board_after_our_move.state_list)\n",
    "\n",
    "    # Terminal after our move?\n",
    "    if 4 in feats_after_our_move:\n",
    "        return 1 * player  # We win\n",
    "    elif -4 in feats_after_our_move:\n",
    "        return -1 * player  # We lose (shouldn't happen)\n",
    "    elif position_i + 1 >= len(moves):\n",
    "        return 0  # Draw\n",
    "\n",
    "    # Check if game ends after opponent's move (position i+2)\n",
    "    if position_i + 2 > len(moves):\n",
    "        return 0  # Draw\n",
    "\n",
    "    board_after_opp = BoardProcessor()\n",
    "    board_after_opp.generate_state_list(moves[:position_i+2])\n",
    "    _, feats_after_opp = feature_gen.convolution_feature_gen(board_after_opp.state_list)\n",
    "\n",
    "    if 4 in feats_after_opp or -4 in feats_after_opp:\n",
    "        return -1   # Opponent wins\n",
    "\n",
    "    # Non-terminal: calculate Q-value of next state\n",
    "    next_board = BoardProcessor()\n",
    "    next_board.generate_state_list(moves[:position_i+2])\n",
    "    _, next_curr_feats = feature_gen.convolution_feature_gen(next_board.state_list)\n",
    "\n",
    "    # Get Q-values for all possible next moves using ONLINE network for selection\n",
    "    online_q_values = []\n",
    "    for col in range(7):\n",
    "        if len(next_board.state_list[col]) < 6:  # Legal move\n",
    "            next_state = [c[:] for c in next_board.state_list]\n",
    "            next_state[col].append(player)  # Same player's turn\n",
    "            _, next_feats = feature_gen.convolution_feature_gen(next_state)\n",
    "\n",
    "            # Check for immediate win\n",
    "            if 4 * player in next_feats:\n",
    "                online_q_values.append((col, 1.0))\n",
    "            else:\n",
    "                # Get Q-value from ONLINE model\n",
    "                features = np.concatenate([next_curr_feats, next_feats])\n",
    "                scaled = scaler.transform([features])\n",
    "                with torch.no_grad():\n",
    "                    q = online_model(torch.FloatTensor(scaled).to(online_model.device if hasattr(online_model, 'device') else 'cpu')).item() * player\n",
    "                    online_q_values.append((col, q))\n",
    "\n",
    "    if not online_q_values:\n",
    "        return 0  # No legal moves = draw\n",
    "\n",
    "    # DOUBLE DQN: Online network selects best action\n",
    "    best_action = max(online_q_values, key=lambda x: x[1])[0]\n",
    "\n",
    "    # TARGET network evaluates the selected action\n",
    "    best_next_state = [c[:] for c in next_board.state_list]\n",
    "    best_next_state[best_action].append(player)\n",
    "    _, best_next_feats = feature_gen.convolution_feature_gen(best_next_state)\n",
    "\n",
    "    # Check for immediate win with selected action\n",
    "    if 4 * player in best_next_feats:\n",
    "        target_q_value = 1.0\n",
    "    else:\n",
    "        # Evaluate using TARGET network\n",
    "        best_features = np.concatenate([next_curr_feats, best_next_feats])\n",
    "        best_scaled = scaler.transform([best_features])\n",
    "        with torch.no_grad():\n",
    "            target_q_value = target_model(torch.FloatTensor(best_scaled).to(target_model.device if hasattr(target_model, 'device') else 'cpu')).item() * player\n",
    "\n",
    "    return gamma * target_q_value\n",
    "\n",
    "def generate_training_tuples(game_codes, online_model, target_model, scaler, feature_gen, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"Generate (state_action_features, target_q) tuples from game codes using Double DQN\"\"\"\n",
    "    training_tuples = []\n",
    "\n",
    "    for game_code in game_codes:\n",
    "        board = BoardProcessor()\n",
    "        moves = board.decode_moves_code(game_code)\n",
    "        board.generate_state_list(moves)\n",
    "\n",
    "        # Process each non-terminal position\n",
    "        for i in range(len(moves) - 1):  # Skip final position\n",
    "            # Current state and player\n",
    "            temp_board = BoardProcessor()\n",
    "            temp_board.generate_state_list(moves[:i])\n",
    "            player = 1 if (i % 2) == 0 else -1\n",
    "\n",
    "            # Get current state features\n",
    "            _, curr_feats = feature_gen.convolution_feature_gen(temp_board.state_list)\n",
    "\n",
    "            # Action taken and resulting state\n",
    "            action = moves[i]\n",
    "            next_state = [col[:] for col in temp_board.state_list]\n",
    "            next_state[action].append(player)\n",
    "            _, next_feats = feature_gen.convolution_feature_gen(next_state)\n",
    "\n",
    "            # Create state-action input features\n",
    "            state_action_features = np.concatenate([curr_feats, next_feats])\n",
    "\n",
    "            # Calculate target Q-value using Double DQN TD update\n",
    "            current_q = get_current_q(state_action_features, online_model, scaler, player)\n",
    "            target_q = calculate_target_q(moves, i, player, online_model, target_model, scaler, feature_gen, gamma)\n",
    "            new_q = current_q + alpha * (target_q - current_q)\n",
    "\n",
    "            training_tuples.append((state_action_features, new_q))\n",
    "\n",
    "            if len(training_tuples) >= 512:\n",
    "                return training_tuples[:512]\n",
    "\n",
    "    return training_tuples"
   ],
   "id": "92ad46808712824e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_generate_training_tuples(skip=0):\n",
    "    \"\"\"Test generate_training_tuples and compare with show_double_dqn_updates logic\"\"\"\n",
    "\n",
    "    # Load models and setup (same as show_double_dqn_updates)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(\"qnet_mc_pretrained.pth\", map_location=device)\n",
    "\n",
    "    # Online and Target networks\n",
    "    online_net = QNetwork().to(device)\n",
    "    online_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    online_net.eval()\n",
    "    online_net.device = device  # Add device attribute for the function\n",
    "\n",
    "    target_net = QNetwork().to(device)\n",
    "    target_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    target_net.eval()\n",
    "    target_net.device = device\n",
    "\n",
    "    scaler = checkpoint['scaler']\n",
    "    feature_gen = FeatureGenerator()\n",
    "\n",
    "    # Test parameters\n",
    "    alpha = 0.1\n",
    "    gamma = 0.99\n",
    "\n",
    "    # Read test game codes\n",
    "    codes_file = os.path.expanduser('~/Downloads/replayMem.txt')\n",
    "    test_codes = []\n",
    "\n",
    "    with open(codes_file, 'r') as f:\n",
    "        print(\"Skipping a million rows...\")\n",
    "        for _ in range(skip):\n",
    "            f.readline()\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 1:  # Take first 5 games\n",
    "                break\n",
    "            test_codes.append(line.strip())\n",
    "\n",
    "    print(f\"Testing with {len(test_codes)} game codes\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Test each game code\n",
    "    for game_idx, game_code in enumerate(test_codes):\n",
    "        print(f\"\\nGAME {game_idx + 1}: {game_code}\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "        # Generate training tuples\n",
    "        tuples = generate_training_tuples(\n",
    "            [game_code],\n",
    "            online_net,\n",
    "            target_net,\n",
    "            scaler,\n",
    "            feature_gen,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma\n",
    "        )\n",
    "\n",
    "        # Decode game to show context\n",
    "        board = BoardProcessor()\n",
    "        moves = board.decode_moves_code(game_code)\n",
    "        print(f\"Total moves: {len(moves)}\")\n",
    "        # print(f\"Last moves: {moves[-3:]}\")\n",
    "        print(f\"Last moves: {'; '.join([('X' if (len(moves)-3+i) % 2 == 0 else 'O') + ':' + str(moves[-3+i]) + (' [final]' if i == 2 else '') for i in range(min(3, len(moves)))])}\")\n",
    "        print(f\"Generated tuples: {len(tuples)}\")\n",
    "\n",
    "        # Show details for last few positions (like show_double_dqn_updates does)\n",
    "        num_positions_to_show = min(3, len(tuples))\n",
    "        start_idx = max(0, len(tuples) - num_positions_to_show)\n",
    "\n",
    "        for i in range(start_idx, len(tuples)):\n",
    "            print(f\"\\n  Position {i+1}/{len(moves)-1}:\")\n",
    "\n",
    "            # Recreate the position to verify\n",
    "            temp_board = BoardProcessor()\n",
    "            temp_board.generate_state_list(moves[:i])\n",
    "            player = 1 if (i % 2) == 0 else -1\n",
    "            action = moves[i]\n",
    "\n",
    "            # Get the tuple\n",
    "            state_action_features, target_q = tuples[i]\n",
    "\n",
    "            # Manually calculate what we expect our action is from replay memory\n",
    "            _, curr_feats = feature_gen.convolution_feature_gen(temp_board.state_list)\n",
    "            next_state = [col[:] for col in temp_board.state_list]\n",
    "            next_state[action].append(player)\n",
    "            _, next_feats = feature_gen.convolution_feature_gen(next_state)\n",
    "\n",
    "            expected_features = np.concatenate([curr_feats, next_feats])\n",
    "\n",
    "            # Verify features match\n",
    "            features_match = np.allclose(state_action_features, expected_features)\n",
    "\n",
    "            # Calculate current Q for comparison\n",
    "            current_q = get_current_q(state_action_features, online_net, scaler, player)\n",
    "\n",
    "            # Calculate what target should be\n",
    "            expected_target = calculate_target_q(\n",
    "                moves, i, player, online_net, target_net, scaler, feature_gen, gamma\n",
    "            )\n",
    "\n",
    "            # The new Q after TD update\n",
    "            expected_new_q = current_q + alpha * (expected_target - current_q)\n",
    "\n",
    "            print(f\"    Player: {'X' if player == 1 else 'O'}, Action: {action}\")\n",
    "            temp_board.display_board()\n",
    "            print(f\"    Features match: {features_match}\")\n",
    "            print(f\"    Current Q: {current_q:+.4f}\")\n",
    "            print(f\"    Target Q (raw): {expected_target:+.4f}\")\n",
    "            print(f\"    Expected new Q: {expected_new_q:+.4f}\")\n",
    "            print(f\"    Actual tuple Q: {target_q:+.4f}\")\n",
    "            print(f\"    Match: {np.isclose(target_q, expected_new_q)}\")\n",
    "\n",
    "            if not np.isclose(target_q, expected_new_q):\n",
    "                print(f\"    ⚠️  MISMATCH DETECTED!\")\n",
    "                print(f\"    Difference: {abs(target_q - expected_new_q):.6f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Batch test for performance\n",
    "    print(f\"\\nBatch testing all {len(test_codes)} games...\")\n",
    "    all_tuples = generate_training_tuples(\n",
    "        test_codes,\n",
    "        online_net,\n",
    "        target_net,\n",
    "        scaler,\n",
    "        feature_gen,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma\n",
    "    )\n",
    "\n",
    "    print(f\"Total tuples generated: {len(all_tuples)}\")\n",
    "    print(f\"Max tuples limit: 512\")\n",
    "    print(f\"Actually returned: {min(len(all_tuples), 512)}\")\n",
    "\n",
    "    # Sample check of Q-value distribution\n",
    "    if all_tuples:\n",
    "        q_values = [q for _, q in all_tuples[:512]]\n",
    "        print(f\"\\nQ-value statistics:\")\n",
    "        print(f\"  Min: {min(q_values):+.4f}\")\n",
    "        print(f\"  Max: {max(q_values):+.4f}\")\n",
    "        print(f\"  Mean: {np.mean(q_values):+.4f}\")\n",
    "        print(f\"  Std: {np.std(q_values):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing generate_training_tuples consistency with show_double_dqn_updates\\n\")\n",
    "    test_generate_training_tuples(skip = 72)"
   ],
   "id": "5295b1d3a14ceb4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def show_double_dqn_updates(game_code, alpha=0.1, gamma=0.99):\n",
    "    # Load models\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(\"qnet_mc_pretrained.pth\", map_location=device)\n",
    "\n",
    "    # Online and Target networks (for now, same weights)\n",
    "    online_net = QNetwork().to(device)\n",
    "    online_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    online_net.eval()\n",
    "\n",
    "    target_net = QNetwork().to(device)\n",
    "    target_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    target_net.eval()\n",
    "\n",
    "    scaler = checkpoint['scaler']\n",
    "\n",
    "    # Setup\n",
    "    board = BoardProcessor()\n",
    "    feature_gen = FeatureGenerator()\n",
    "    moves = board.decode_moves_code(game_code)\n",
    "    board.generate_state_list(moves)\n",
    "\n",
    "    print(f\"Game: {game_code} ({len(moves)} moves)\")\n",
    "\n",
    "    # Show last few positions with Double DQN updates\n",
    "    for i in range(max(0, len(moves)-5), len(moves)-1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"POSITION {i+1}/{len(moves)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        board.display_board(index=i-1)\n",
    "\n",
    "        # Get current state\n",
    "        temp_board = BoardProcessor()\n",
    "        temp_board.generate_state_list(moves[:i])\n",
    "        player = 1 if (i+1) % 2 == 1 else -1\n",
    "\n",
    "        # Calculate ALL Q-values for current position (online network)\n",
    "        _, curr_feats = feature_gen.convolution_feature_gen(temp_board.state_list)\n",
    "        q_vals = []\n",
    "        q_dict = {}\n",
    "\n",
    "        for col in range(7):\n",
    "            if len(temp_board.state_list[col]) < 6:  # Legal move\n",
    "                next_state = [c[:] for c in temp_board.state_list]\n",
    "                next_state[col].append(player)\n",
    "                board_matrix, next_feats = feature_gen.convolution_feature_gen(next_state)\n",
    "                # print(board_matrix)\n",
    "\n",
    "                features = np.concatenate([curr_feats, next_feats])\n",
    "                scaled = scaler.transform([features])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    q = online_net(torch.FloatTensor(scaled).to(device)).item() * player\n",
    "                    q_dict[col] = q\n",
    "                    q_vals.append(f\"{q:+.3f}\")\n",
    "            else:\n",
    "                q_vals.append(\" --- \")\n",
    "\n",
    "        print(f\"\\nPlayer {'X' if player == 1 else 'O'} to move\")\n",
    "        print(f\"Online Q-values for all actions:\")\n",
    "        print(f\"Columns: 0      1      2      3      4      5      6\")\n",
    "        print(f\"Q(s,a):  {' '.join(q_vals)}\")\n",
    "\n",
    "        # Action taken (from replay memory)\n",
    "        action = moves[i]\n",
    "        print(f\"\\nAction taken: Column {action} (Q={q_dict[action]:+.4f})\")\n",
    "\n",
    "        # Current Q-value for the taken action\n",
    "        current_q = q_dict[action]\n",
    "\n",
    "        # Check if move i+1 ends the game\n",
    "        board_after_our_move = BoardProcessor()\n",
    "        board_after_our_move.generate_state_list(moves[:i+2])\n",
    "        _, feats_after_our_move = feature_gen.convolution_feature_gen(board_after_our_move.state_list)\n",
    "\n",
    "        # Did our move end the game?\n",
    "        if 4 in feats_after_our_move or -4 in feats_after_our_move or i+2 >= len(moves):\n",
    "            # Terminal immediately after our move\n",
    "            print(\"^\"*15+ f\"Debug This is allegedly terminal? if {i} plus two is greater or equal to {len(moves)}\")\n",
    "\n",
    "            if 4 in feats_after_our_move:\n",
    "                reward = 1 * player  # We made 4-in-a-row\n",
    "            elif -4 in feats_after_our_move:\n",
    "                reward = -1 * player  # We somehow made opponent's 4-in-a-row (shouldn't happen)\n",
    "            else:\n",
    "                reward = 0  # Draw\n",
    "\n",
    "            target_value = reward\n",
    "            print(f\"\\n--- Double DQN Update ---\")\n",
    "            print(f\"Terminal after our move, reward: {reward:+.4f}\")\n",
    "\n",
    "        else:\n",
    "            # Non-terminal after our move\n",
    "            reward = 0  # No intermediate rewards\n",
    "\n",
    "            # Next state is from OPPONENT's perspective after they move\n",
    "            # But for Q-learning, we care about OUR next state (after opponent moves)\n",
    "\n",
    "            # The key insight: we need the value of the state AFTER opponent moves\n",
    "            # This is position i+3 from our perspective\n",
    "            print(\"^\"*15+ f\"Debug not yet terminal {i} plus two is greater or equal to {len(moves)}\")\n",
    "\n",
    "            # else:\n",
    "            # Game continues - evaluate our position after opponent moves\n",
    "            next_board = BoardProcessor()\n",
    "            next_board.generate_state_list(moves[:i+2])\n",
    "            _, next_curr_feats = feature_gen.convolution_feature_gen(next_board.state_list)\n",
    "\n",
    "            # Get Q-values for OUR next move (same player perspective)\n",
    "            online_next_q_values = []\n",
    "            for col in range(7):\n",
    "                if len(next_board.state_list[col]) < 6:\n",
    "                    ns = [c[:] for c in next_board.state_list]\n",
    "                    ns[col].append(player)  # Same player\n",
    "                    _, ns_feats = feature_gen.convolution_feature_gen(ns)\n",
    "                    # print(brdd,ns_feats, ns)\n",
    "                    features = np.concatenate([next_curr_feats, ns_feats])\n",
    "                    scaled = scaler.transform([features])\n",
    "                    q = 1 if 4 * player in ns_feats else (0 if i + 3 >= 42 else online_net(torch.FloatTensor(scaled).to(device)).item() * player)\n",
    "                    # with torch.no_grad():\n",
    "                    #     #Now if one of the moves is winning - skip online_net and just fix Q\n",
    "                    #     q = 1 if 4 * player in ns_feats else online_net(torch.FloatTensor(scaled).to(device)).item() * player\n",
    "                    #     #But if we run out of board?\n",
    "                    #     if i + 3 >= 42: q=0\n",
    "                    online_next_q_values.append((col, q))\n",
    "\n",
    "            if not online_next_q_values:\n",
    "                target_value = 0  # No moves = draw\n",
    "            else:\n",
    "                # Double DQN: online selects, target evaluates\n",
    "                best_action = max(online_next_q_values, key=lambda x: x[1])[0]\n",
    "\n",
    "                # Target evaluates\n",
    "                best_ns = [c[:] for c in next_board.state_list]\n",
    "                best_ns[best_action].append(player)\n",
    "                _, best_ns_feats = feature_gen.convolution_feature_gen(best_ns)\n",
    "                best_features = np.concatenate([next_curr_feats, best_ns_feats])\n",
    "                best_scaled = scaler.transform([best_features])\n",
    "                target_q_value = 1 if 4 * player in best_ns_feats else (0 if i + 3 >= 42 else online_net(torch.FloatTensor(best_scaled).to(device)).item() * player)\n",
    "                #\n",
    "                # with torch.no_grad():\n",
    "                #     target_q_value = 1 if 4 * player in best_ns_feats else target_net(torch.FloatTensor(best_scaled).to(device)).item() * player\n",
    "                #     if i + 3 >= 42: target_q_value=0\n",
    "\n",
    "                target_value = reward + gamma * target_q_value\n",
    "\n",
    "                print(f\"\\n--- Double DQN Update ---\")\n",
    "                print(f\"After opponent plays column {moves[i+1]}:\")\n",
    "                print(f\"Our next turn Q-values: {[f'{c}:{q:.3f}' for c,q in online_next_q_values]}\")\n",
    "                print(f\"Online would select: column {best_action}\")\n",
    "                print(f\"Target evaluates: {target_q_value:+.4f}\")\n",
    "                print(f\"Target value: 0 + {gamma:.3f}*{target_q_value:+.4f} = {target_value:+.4f}\")\n",
    "\n",
    "        # TD error and update\n",
    "        td_error = target_value - current_q\n",
    "        new_q = current_q + alpha * td_error\n",
    "\n",
    "        print(f\"\\nTD Error: {target_value:+.4f} - {current_q:+.4f} = {td_error:+.4f}\")\n",
    "        print(f\"Q-update: {current_q:+.4f} + {alpha}*{td_error:+.4f} = {new_q:+.4f}\")\n",
    "        # Add this right after the Q-update print statement\n",
    "        print(f\"\\n--- Verification: State-Action Pair ---\")\n",
    "        print(f\"STATE (before move {action}):\")\n",
    "        temp_board_before = BoardProcessor()\n",
    "        temp_board_before.generate_state_list(moves[:i])\n",
    "        temp_board_before.display_board()\n",
    "\n",
    "        print(f\"\\nACTION: Player {'X' if player == 1 else 'O'} plays column {action}\")\n",
    "\n",
    "        print(f\"\\nRESULTING STATE (after move in position {action}):\")\n",
    "        temp_board_after = BoardProcessor()\n",
    "        temp_board_after.generate_state_list(moves[:i+1])\n",
    "        temp_board_after.display_board()\n",
    "\n",
    "        print(f\"\\nQ-value being updated: Q(state, action={action}) = {current_q:.4f} → {new_q:.4f}\")\n"
   ],
   "id": "2eda4ef9a8fdd10c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "codes_file = os.path.expanduser('~/Downloads/replayMem.txt')",
   "id": "555c08361ba1740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "skip_rows = 72\n",
    "skip_rows = int(skip_rows)\n",
    "with open(codes_file, 'r') as f:\n",
    "    print(f\"Skipping a {skip_rows} rows\")\n",
    "    for _ in range(skip_rows):\n",
    "        f.readline()\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 1:  # Only take first 5\n",
    "            break\n",
    "        code = line.strip()\n",
    "        show_double_dqn_updates(code)\n",
    "# Usage\n",
    "# with open(\"game_codes.txt\") as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         if i < 2:  # First 2 games\n",
    "#             print(f\"\\n{'#'*60}\")\n",
    "#             print(f\"GAME {i+1}\")\n",
    "#             print(f\"{'#'*60}\")\n",
    "#             show_double_dqn_updates(line.strip())"
   ],
   "id": "d18b2cf34e61c9a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(os.path.expanduser('~/Downloads/replayMem.txt'), 'r') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        code = line.strip()\n",
    "        if code:\n",
    "            board = BoardProcessor()\n",
    "            try:\n",
    "                moves = board.decode_moves_code(code)\n",
    "                if len(moves) == 42:\n",
    "                    print(f\"First draw found at line {line_num}\")\n",
    "                    print(f\"Game code: {code}\")\n",
    "                    print(f\"Code length: {len(code)} characters\")\n",
    "                    print(f\"Moves: {len(moves)}\")\n",
    "                    # Optionally show the final position\n",
    "                    board.generate_state_list(moves)\n",
    "                    board.display_board()\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"No draw games found!\")"
   ],
   "id": "479bb2744afcb243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#debugging this:\n",
    "board = BoardProcessor()\n",
    "moves = board.decode_moves_code('8IIa2rwOJR')\n",
    "board.generate_state_list(moves[:])\n",
    "board.display_board()\n",
    "feature_gen = FeatureGenerator()\n",
    "feature_gen.convolution_feature_gen(board.state_list)"
   ],
   "id": "6b7ff641ebc0d27f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"Section 1: Let's take a look at target Q-values\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from board_processor import BoardProcessor\n",
    "from feature_generator import FeatureGenerator\n",
    "import os"
   ],
   "id": "60af60431e8daa3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=138):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for h in [256, 128, 64, 32, 16, 8]:\n",
    "            layers.extend([nn.Linear(input_dim, h), nn.Tanh()])\n",
    "            input_dim = h\n",
    "        layers.extend([nn.Linear(h, 1), nn.Tanh()])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)"
   ],
   "id": "e70f10b08be0cafb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def show_q_values(game_code):\n",
    "    # Load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(\"qnet_mc_pretrained.pth\", map_location=device)\n",
    "    model = QNetwork().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    scaler = checkpoint['scaler']\n",
    "\n",
    "    # Setup\n",
    "    board = BoardProcessor()\n",
    "    feature_gen = FeatureGenerator()\n",
    "    moves = board.decode_moves_code(game_code)\n",
    "    board.generate_state_list(moves)\n",
    "\n",
    "    # Show last 3 positions (2 with Q-values, 1 terminal)\n",
    "    for i in range(max(0, len(moves)-3), len(moves)):\n",
    "        print(f\"\\n--- Position {i+1}/{len(moves)} ---\")\n",
    "        board.display_board(index=i)\n",
    "\n",
    "        if i < len(moves)-1:  # Not terminal\n",
    "            # Get state and calculate Q-values\n",
    "            temp_board = BoardProcessor()\n",
    "            temp_board.generate_state_list(moves[:i+1])\n",
    "            player = 1 if (i+1) % 2 == 1 else -1\n",
    "\n",
    "            _, curr_feats = feature_gen.convolution_feature_gen(temp_board.state_list)\n",
    "            q_vals = []\n",
    "\n",
    "            for col in range(7):\n",
    "                if len(temp_board.state_list[col]) < 6:  # Legal move\n",
    "                    next_state = [c[:] for c in temp_board.state_list]\n",
    "                    next_state[col].append(player)\n",
    "                    _, next_feats = feature_gen.convolution_feature_gen(next_state)\n",
    "\n",
    "                    features = np.concatenate([curr_feats, next_feats])\n",
    "                    scaled = scaler.transform([features])\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        q = model(torch.FloatTensor(scaled).to(device)).item() * player\n",
    "                        q_vals.append(f\"{q:+.2f}\")\n",
    "                else:\n",
    "                    q_vals.append(\" --- \")\n",
    "\n",
    "            print(f\"Q-values: {' '.join(q_vals)}\")\n",
    "            print(f\"Next move: {moves[i+1]} (Q={q_vals[moves[i+1]].strip()})\")"
   ],
   "id": "34408f706a6daf70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "codes_file = os.path.expanduser('~/Downloads/replayMem.txt')",
   "id": "c6b70feb01cbb014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "with open(codes_file, 'r') as f:\n",
    "    print(\"Skipping a million rows\")\n",
    "    for _ in range(1_000_000):\n",
    "        f.readline()\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:  # Only take first 5\n",
    "            break\n",
    "        code = line.strip()\n",
    "        show_q_values(code)"
   ],
   "id": "25e13c8fe8d54c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc5d1e8243eefb5b",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Display last 3 moves from game codes using existing board utilities.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from board_processor import BoardProcessor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def display_last_moves(game_code, num_last_moves=3):\n",
    "    \"\"\"\n",
    "    Display the last N moves of a game.\n",
    "\n",
    "    Args:\n",
    "        game_code: String game code\n",
    "        num_last_moves: Number of final moves to display\n",
    "    \"\"\"\n",
    "    # Initialize board processor\n",
    "    board = BoardProcessor()\n",
    "\n",
    "    # Decode the game code to get move sequence\n",
    "    try:\n",
    "        moves = board.decode_moves_code(game_code)\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding {game_code}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Generate the full game\n",
    "    board.generate_state_list(moves)\n",
    "\n",
    "    # Determine game outcome\n",
    "    total_moves = len(moves)\n",
    "\n",
    "    # Display game info\n",
    "    print(f\"\\nGame Code: {game_code}\")\n",
    "    print(f\"Total moves: {total_moves}\")\n",
    "\n",
    "    # Calculate starting position for last N moves\n",
    "    start_position = max(0, total_moves - num_last_moves)\n",
    "\n",
    "    # Display last N positions\n",
    "    for i in range(start_position, total_moves):\n",
    "        print(f\"\\n--- After move {i + 1} (column {moves[i]}) ---\")\n",
    "        board.display_board(index=i)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)"
   ],
   "id": "817a652188c9776f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def main(replay_mem_filename):\n",
    "    # Path to game codes file\n",
    "    codes_file = os.path.expanduser(replay_mem_filename)\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(codes_file):\n",
    "        print(f\"File not found: {codes_file}\")\n",
    "        print(\"Please ensure game_codes.txt is in your Downloads folder\")\n",
    "        return\n",
    "\n",
    "    # Read first 5 game codes\n",
    "    print(\"Loading first 5 games from file...\")\n",
    "    game_codes = []\n",
    "\n",
    "    with open(codes_file, 'r') as f:\n",
    "        print(\"Skipping a million rows\")\n",
    "        for _ in range(1_000_000):\n",
    "            f.readline()\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 5:  # Only take first 5\n",
    "                break\n",
    "            code = line.strip()\n",
    "            if code:  # Skip empty lines\n",
    "                game_codes.append(code)\n",
    "\n",
    "    print(f\"Found {len(game_codes)} game codes\")\n",
    "\n",
    "    # Display last 3 moves for each game\n",
    "    for idx, code in enumerate(game_codes, 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"GAME {idx} OF 5\")\n",
    "        print(f\"{'='*40}\")\n",
    "        display_last_moves(code, num_last_moves=3)"
   ],
   "id": "9f0640d3d76919cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"~/Downloads/replayMem.txt\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "18b82d9d4fdaba30",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
