{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from board_processor import BoardProcessor\n",
    "from feature_generator import FeatureGenerator\n",
    "import os\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=138):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for h in [256, 128, 64, 32, 16, 8]:\n",
    "            layers.extend([nn.Linear(input_dim, h), nn.Tanh()])\n",
    "            input_dim = h\n",
    "        layers.extend([nn.Linear(h, 1), nn.Tanh()])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)"
   ],
   "id": "8d39ada4cd0686eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def show_double_dqn_updates(game_code, alpha=0.1, gamma=0.99):\n",
    "    # Load models\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(\"qnet_mc_pretrained.pth\", map_location=device)\n",
    "\n",
    "    # Online and Target networks (for now, same weights)\n",
    "    online_net = QNetwork().to(device)\n",
    "    online_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    online_net.eval()\n",
    "\n",
    "    target_net = QNetwork().to(device)\n",
    "    target_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    target_net.eval()\n",
    "\n",
    "    scaler = checkpoint['scaler']\n",
    "\n",
    "    # Setup\n",
    "    board = BoardProcessor()\n",
    "    feature_gen = FeatureGenerator()\n",
    "    moves = board.decode_moves_code(game_code)\n",
    "    board.generate_state_list(moves)\n",
    "\n",
    "    print(f\"Game: {game_code} ({len(moves)} moves)\")\n",
    "\n",
    "    # Show last few positions with Double DQN updates\n",
    "    for i in range(max(0, len(moves)-5), len(moves)-1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"POSITION {i+1}/{len(moves)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        board.display_board(index=i-1)\n",
    "\n",
    "        # Get current state\n",
    "        temp_board = BoardProcessor()\n",
    "        temp_board.generate_state_list(moves[:i])\n",
    "        player = 1 if (i+1) % 2 == 1 else -1\n",
    "\n",
    "        # Calculate ALL Q-values for current position (online network)\n",
    "        _, curr_feats = feature_gen.convolution_feature_gen(temp_board.state_list)\n",
    "        q_vals = []\n",
    "        q_dict = {}\n",
    "\n",
    "        for col in range(7):\n",
    "            if len(temp_board.state_list[col]) < 6:  # Legal move\n",
    "                next_state = [c[:] for c in temp_board.state_list]\n",
    "                next_state[col].append(player)\n",
    "                board_matrix, next_feats = feature_gen.convolution_feature_gen(next_state)\n",
    "                # print(board_matrix)\n",
    "\n",
    "                features = np.concatenate([curr_feats, next_feats])\n",
    "                scaled = scaler.transform([features])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    q = online_net(torch.FloatTensor(scaled).to(device)).item() * player\n",
    "                    q_dict[col] = q\n",
    "                    q_vals.append(f\"{q:+.3f}\")\n",
    "            else:\n",
    "                q_vals.append(\" --- \")\n",
    "\n",
    "        print(f\"\\nPlayer {'X' if player == 1 else 'O'} to move\")\n",
    "        print(f\"Online Q-values for all actions:\")\n",
    "        print(f\"Columns: 0      1      2      3      4      5      6\")\n",
    "        print(f\"Q(s,a):  {' '.join(q_vals)}\")\n",
    "\n",
    "        # Action taken (from replay memory)\n",
    "        action = moves[i]\n",
    "        print(f\"\\nAction taken: Column {action} (Q={q_dict[action]:+.4f})\")\n",
    "\n",
    "        # Current Q-value for the taken action\n",
    "        current_q = q_dict[action]\n",
    "\n",
    "        # Check if move i+1 ends the game\n",
    "        board_after_our_move = BoardProcessor()\n",
    "        board_after_our_move.generate_state_list(moves[:i+2])\n",
    "        _, feats_after_our_move = feature_gen.convolution_feature_gen(board_after_our_move.state_list)\n",
    "\n",
    "        # Did our move end the game?\n",
    "        if 4 in feats_after_our_move or -4 in feats_after_our_move or i+2 >= len(moves):\n",
    "            # Terminal immediately after our move\n",
    "            if 4 in feats_after_our_move:\n",
    "                reward = 1 * player  # We made 4-in-a-row\n",
    "            elif -4 in feats_after_our_move:\n",
    "                reward = -1 * player  # We somehow made opponent's 4-in-a-row (shouldn't happen)\n",
    "            else:\n",
    "                reward = 0  # Draw\n",
    "\n",
    "            target_value = reward\n",
    "            print(f\"\\n--- Double DQN Update ---\")\n",
    "            print(f\"Terminal after our move, reward: {reward:+.4f}\")\n",
    "\n",
    "        else:\n",
    "            # Non-terminal after our move\n",
    "            reward = 0  # No intermediate rewards\n",
    "\n",
    "            # Next state is from OPPONENT's perspective after they move\n",
    "            # But for Q-learning, we care about OUR next state (after opponent moves)\n",
    "\n",
    "            # The key insight: we need the value of the state AFTER opponent moves\n",
    "            # This is position i+3 from our perspective\n",
    "\n",
    "            if i+2 > len(moves):\n",
    "                # Opponent's move ends the game\n",
    "                board_after_opp = BoardProcessor()\n",
    "                board_after_opp.generate_state_list(moves[:i+2])\n",
    "                _, feats_after_opp = feature_gen.convolution_feature_gen(board_after_opp.state_list)\n",
    "\n",
    "                if 4 in feats_after_opp or -4 in feats_after_opp:\n",
    "                    # Game ended - we lost (opponent won)\n",
    "                    target_value = -1 * player\n",
    "                else:\n",
    "                    target_value = 0  # Draw\n",
    "\n",
    "                print(f\"\\n--- Double DQN Update ---\")\n",
    "                print(f\"Terminal after opponent's move {moves[i+2]}, reward: {target_value:+.4f}\")\n",
    "\n",
    "            else:\n",
    "                # Game continues - evaluate our position after opponent moves\n",
    "                next_board = BoardProcessor()\n",
    "                next_board.generate_state_list(moves[:i+2])\n",
    "                _, next_curr_feats = feature_gen.convolution_feature_gen(next_board.state_list)\n",
    "\n",
    "                # Get Q-values for OUR next move (same player perspective)\n",
    "                online_next_q_values = []\n",
    "                for col in range(7):\n",
    "                    if len(next_board.state_list[col]) < 6:\n",
    "                        ns = [c[:] for c in next_board.state_list]\n",
    "                        ns[col].append(player)  # Same player\n",
    "                        _, ns_feats = feature_gen.convolution_feature_gen(ns)\n",
    "                        # print(brdd,ns_feats, ns)\n",
    "                        features = np.concatenate([next_curr_feats, ns_feats])\n",
    "                        scaled = scaler.transform([features])\n",
    "                        with torch.no_grad():\n",
    "                            #Now if one of the moves is winning - skip online_net and just fix Q\n",
    "                            q = 1 if 4 * player in ns_feats else online_net(torch.FloatTensor(scaled).to(device)).item() * player\n",
    "\n",
    "                            online_next_q_values.append((col, q))\n",
    "\n",
    "                if not online_next_q_values:\n",
    "                    target_value = 0  # No moves = draw\n",
    "                else:\n",
    "                    # Double DQN: online selects, target evaluates\n",
    "                    best_action = max(online_next_q_values, key=lambda x: x[1])[0]\n",
    "\n",
    "                    # Target evaluates\n",
    "                    best_ns = [c[:] for c in next_board.state_list]\n",
    "                    best_ns[best_action].append(player)\n",
    "                    _, best_ns_feats = feature_gen.convolution_feature_gen(best_ns)\n",
    "                    best_features = np.concatenate([next_curr_feats, best_ns_feats])\n",
    "                    best_scaled = scaler.transform([best_features])\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        target_q_value = 1 if 4 * player in best_ns_feats else target_net(torch.FloatTensor(best_scaled).to(device)).item() * player\n",
    "\n",
    "                    target_value = reward + gamma * target_q_value\n",
    "\n",
    "                    print(f\"\\n--- Double DQN Update ---\")\n",
    "                    print(f\"After opponent plays column {moves[i+1]}:\")\n",
    "                    print(f\"Our next turn Q-values: {[f'{c}:{q:.3f}' for c,q in online_next_q_values]}\")\n",
    "                    print(f\"Online would select: column {best_action}\")\n",
    "                    print(f\"Target evaluates: {target_q_value:+.4f}\")\n",
    "                    print(f\"Target value: 0 + {gamma:.3f}*{target_q_value:+.4f} = {target_value:+.4f}\")\n",
    "\n",
    "        # TD error and update\n",
    "        td_error = target_value - current_q\n",
    "        new_q = current_q + alpha * td_error\n",
    "\n",
    "        print(f\"\\nTD Error: {target_value:+.4f} - {current_q:+.4f} = {td_error:+.4f}\")\n",
    "        print(f\"Q-update: {current_q:+.4f} + {alpha}*{td_error:+.4f} = {new_q:+.4f}\")\n"
   ],
   "id": "2eda4ef9a8fdd10c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "codes_file = os.path.expanduser('~/Downloads/replayMem.txt')",
   "id": "555c08361ba1740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "with open(codes_file, 'r') as f:\n",
    "    print(\"Skipping a million rows\")\n",
    "    for _ in range(1_000_000):\n",
    "        f.readline()\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 1:  # Only take first 5\n",
    "            break\n",
    "        code = line.strip()\n",
    "        show_double_dqn_updates(code)\n",
    "# Usage\n",
    "# with open(\"game_codes.txt\") as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         if i < 2:  # First 2 games\n",
    "#             print(f\"\\n{'#'*60}\")\n",
    "#             print(f\"GAME {i+1}\")\n",
    "#             print(f\"{'#'*60}\")\n",
    "#             show_double_dqn_updates(line.strip())"
   ],
   "id": "d18b2cf34e61c9a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#debugging this:\n",
    "board = BoardProcessor()\n",
    "moves = board.decode_moves_code('8IIa2rwOJR')\n",
    "board.generate_state_list(moves[:])\n",
    "board.display_board()\n",
    "feature_gen = FeatureGenerator()\n",
    "feature_gen.convolution_feature_gen(board.state_list)"
   ],
   "id": "6b7ff641ebc0d27f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"Section 1: Let's take a look at target Q-values\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from board_processor import BoardProcessor\n",
    "from feature_generator import FeatureGenerator\n",
    "import os"
   ],
   "id": "60af60431e8daa3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=138):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for h in [256, 128, 64, 32, 16, 8]:\n",
    "            layers.extend([nn.Linear(input_dim, h), nn.Tanh()])\n",
    "            input_dim = h\n",
    "        layers.extend([nn.Linear(h, 1), nn.Tanh()])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)"
   ],
   "id": "e70f10b08be0cafb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def show_q_values(game_code):\n",
    "    # Load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(\"qnet_mc_pretrained.pth\", map_location=device)\n",
    "    model = QNetwork().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    scaler = checkpoint['scaler']\n",
    "\n",
    "    # Setup\n",
    "    board = BoardProcessor()\n",
    "    feature_gen = FeatureGenerator()\n",
    "    moves = board.decode_moves_code(game_code)\n",
    "    board.generate_state_list(moves)\n",
    "\n",
    "    # Show last 3 positions (2 with Q-values, 1 terminal)\n",
    "    for i in range(max(0, len(moves)-3), len(moves)):\n",
    "        print(f\"\\n--- Position {i+1}/{len(moves)} ---\")\n",
    "        board.display_board(index=i)\n",
    "\n",
    "        if i < len(moves)-1:  # Not terminal\n",
    "            # Get state and calculate Q-values\n",
    "            temp_board = BoardProcessor()\n",
    "            temp_board.generate_state_list(moves[:i+1])\n",
    "            player = 1 if (i+1) % 2 == 1 else -1\n",
    "\n",
    "            _, curr_feats = feature_gen.convolution_feature_gen(temp_board.state_list)\n",
    "            q_vals = []\n",
    "\n",
    "            for col in range(7):\n",
    "                if len(temp_board.state_list[col]) < 6:  # Legal move\n",
    "                    next_state = [c[:] for c in temp_board.state_list]\n",
    "                    next_state[col].append(player)\n",
    "                    _, next_feats = feature_gen.convolution_feature_gen(next_state)\n",
    "\n",
    "                    features = np.concatenate([curr_feats, next_feats])\n",
    "                    scaled = scaler.transform([features])\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        q = model(torch.FloatTensor(scaled).to(device)).item() * player\n",
    "                        q_vals.append(f\"{q:+.2f}\")\n",
    "                else:\n",
    "                    q_vals.append(\" --- \")\n",
    "\n",
    "            print(f\"Q-values: {' '.join(q_vals)}\")\n",
    "            print(f\"Next move: {moves[i+1]} (Q={q_vals[moves[i+1]].strip()})\")"
   ],
   "id": "34408f706a6daf70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "codes_file = os.path.expanduser('~/Downloads/replayMem.txt')",
   "id": "c6b70feb01cbb014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "with open(codes_file, 'r') as f:\n",
    "    print(\"Skipping a million rows\")\n",
    "    for _ in range(1_000_000):\n",
    "        f.readline()\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:  # Only take first 5\n",
    "            break\n",
    "        code = line.strip()\n",
    "        show_q_values(code)"
   ],
   "id": "25e13c8fe8d54c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc5d1e8243eefb5b",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Display last 3 moves from game codes using existing board utilities.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from board_processor import BoardProcessor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def display_last_moves(game_code, num_last_moves=3):\n",
    "    \"\"\"\n",
    "    Display the last N moves of a game.\n",
    "\n",
    "    Args:\n",
    "        game_code: String game code\n",
    "        num_last_moves: Number of final moves to display\n",
    "    \"\"\"\n",
    "    # Initialize board processor\n",
    "    board = BoardProcessor()\n",
    "\n",
    "    # Decode the game code to get move sequence\n",
    "    try:\n",
    "        moves = board.decode_moves_code(game_code)\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding {game_code}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Generate the full game\n",
    "    board.generate_state_list(moves)\n",
    "\n",
    "    # Determine game outcome\n",
    "    total_moves = len(moves)\n",
    "\n",
    "    # Display game info\n",
    "    print(f\"\\nGame Code: {game_code}\")\n",
    "    print(f\"Total moves: {total_moves}\")\n",
    "\n",
    "    # Calculate starting position for last N moves\n",
    "    start_position = max(0, total_moves - num_last_moves)\n",
    "\n",
    "    # Display last N positions\n",
    "    for i in range(start_position, total_moves):\n",
    "        print(f\"\\n--- After move {i + 1} (column {moves[i]}) ---\")\n",
    "        board.display_board(index=i)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)"
   ],
   "id": "817a652188c9776f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def main(replay_mem_filename):\n",
    "    # Path to game codes file\n",
    "    codes_file = os.path.expanduser(replay_mem_filename)\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(codes_file):\n",
    "        print(f\"File not found: {codes_file}\")\n",
    "        print(\"Please ensure game_codes.txt is in your Downloads folder\")\n",
    "        return\n",
    "\n",
    "    # Read first 5 game codes\n",
    "    print(\"Loading first 5 games from file...\")\n",
    "    game_codes = []\n",
    "\n",
    "    with open(codes_file, 'r') as f:\n",
    "        print(\"Skipping a million rows\")\n",
    "        for _ in range(1_000_000):\n",
    "            f.readline()\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 5:  # Only take first 5\n",
    "                break\n",
    "            code = line.strip()\n",
    "            if code:  # Skip empty lines\n",
    "                game_codes.append(code)\n",
    "\n",
    "    print(f\"Found {len(game_codes)} game codes\")\n",
    "\n",
    "    # Display last 3 moves for each game\n",
    "    for idx, code in enumerate(game_codes, 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"GAME {idx} OF 5\")\n",
    "        print(f\"{'='*40}\")\n",
    "        display_last_moves(code, num_last_moves=3)"
   ],
   "id": "9f0640d3d76919cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"~/Downloads/replayMem.txt\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "18b82d9d4fdaba30",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
