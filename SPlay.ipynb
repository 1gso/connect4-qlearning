{
 "cells": [
  {
   "cell_type": "code",
   "id": "6fea6d1280650be3",
   "metadata": {},
   "source": [
    "# Connect 4 Self-Play Arena\n",
    "# Two Q-Networks battle each other for continuous improvement\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from board_processor import BoardProcessor\n",
    "from feature_generator import FeatureGenerator\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "@dataclass\n",
    "class SelfPlayConfig:\n",
    "    \"\"\"Configuration for self-play sessions\"\"\"\n",
    "    model_path_alpha: str = \"qnet_mc_pretrained.pth\"\n",
    "    model_path_bravo: str = \"qnet_mc_pretrained.pth\"\n",
    "    epsilon_alpha: float = 0.1\n",
    "    epsilon_bravo: float = 0.1\n",
    "    alpha_plays_first: bool = True\n",
    "    num_games: int = 100\n",
    "    verbose: bool = True\n",
    "    save_games: bool = True\n",
    "    game_save_path: str = \"selfplay_games.pkl\""
   ],
   "id": "36e449a2dac60716",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "@dataclass\n",
    "class GameResult:\n",
    "    \"\"\"Store results of a single game\"\"\"\n",
    "    moves: List[int]\n",
    "    winner: int  # 1 for alpha, -1 for bravo, 0 for draw\n",
    "    game_length: int\n",
    "    game_code: str\n",
    "    alpha_first: bool\n",
    "    epsilon_alpha: float\n",
    "    epsilon_bravo: float"
   ],
   "id": "69e9f4d77f1e19ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes=(256, 128, 64, 32, 16, 8)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_dim, h))\n",
    "            layers.append(nn.Tanh())\n",
    "            last_dim = h\n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)"
   ],
   "id": "ea7a3b59e59d4710",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class SelfPlayArena:\n",
    "    \"\"\"Manages self-play between two Q-Networks\"\"\"\n",
    "\n",
    "    def __init__(self, config: SelfPlayConfig):\n",
    "        self.config = config\n",
    "        self.feature_gen = FeatureGenerator()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Calculate feature dimensions\n",
    "        _, dummy_features = self.feature_gen.convolution_feature_gen([[] for _ in range(7)])\n",
    "        self.feature_dim = len(dummy_features) * 2  # State-action pairs\n",
    "\n",
    "        # Load models and scalers\n",
    "        self.model_alpha, self.scaler_alpha = self._load_model(config.model_path_alpha)\n",
    "        self.model_bravo, self.scaler_bravo = self._load_model(config.model_path_bravo)\n",
    "\n",
    "        # Game statistics\n",
    "        self.reset_stats()\n",
    "\n",
    "        print(f\"Arena initialized! Using {self.device}\")\n",
    "        print(f\"Feature dimension: {self.feature_dim}\")\n",
    "        print(f\"Alpha model: {config.model_path_alpha}\")\n",
    "        print(f\"Bravo model: {config.model_path_bravo}\")\n",
    "\n",
    "    def _load_model(self, model_path: str) -> Tuple[QNetwork, object]:\n",
    "        \"\"\"Load a model and its scaler\"\"\"\n",
    "        model = QNetwork(input_dim=self.feature_dim).to(self.device)\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        scaler = checkpoint['scaler']\n",
    "        return model, scaler\n",
    "\n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset game statistics\"\"\"\n",
    "        self.stats = {\n",
    "            'alpha_wins': 0,\n",
    "            'bravo_wins': 0,\n",
    "            'draws': 0,\n",
    "            'total_games': 0,\n",
    "            'avg_game_length': 0,\n",
    "            'game_results': []\n",
    "        }\n",
    "\n",
    "    def get_q_value(self, state_list: List[List[int]], move: int, player: int,\n",
    "                   model: QNetwork, scaler) -> float:\n",
    "        \"\"\"Get Q-value for a state-action pair\"\"\"\n",
    "        _, curr_feats = self.feature_gen.convolution_feature_gen(state_list)\n",
    "        next_state = [col[:] for col in state_list]\n",
    "        next_state[move].append(player)\n",
    "        _, next_feats = self.feature_gen.convolution_feature_gen(next_state)\n",
    "\n",
    "        features = np.concatenate([curr_feats, next_feats])\n",
    "        scaled = scaler.transform([features])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return model(torch.FloatTensor(scaled).to(self.device)).item()\n",
    "\n",
    "    def get_ai_move(self, state_list: List[List[int]], player: int, epsilon: float,\n",
    "                   model: QNetwork, scaler) -> Tuple[int, float, Dict[int, float]]:\n",
    "        \"\"\"Select AI move using epsilon-greedy strategy\"\"\"\n",
    "        valid = [c for c in range(7) if len(state_list[c]) < 6]\n",
    "        q_values = {}\n",
    "\n",
    "        # Calculate Q-values for all valid moves\n",
    "        for col in valid:\n",
    "            q = self.get_q_value(state_list, col, player, model, scaler) * player\n",
    "            q_values[col] = q\n",
    "\n",
    "        # Epsilon-greedy selection\n",
    "        if np.random.random() < epsilon:\n",
    "            selected_col = int(np.random.choice(valid))\n",
    "        else:\n",
    "            selected_col = max(q_values.keys(), key=lambda k: q_values[k])\n",
    "\n",
    "        return selected_col, q_values[selected_col], q_values\n",
    "\n",
    "    def check_win(self, state_list: List[List[int]]) -> int:\n",
    "        \"\"\"Check for win using convolution features. Returns 1, -1, or 0\"\"\"\n",
    "        _, features = self.feature_gen.convolution_feature_gen(state_list)\n",
    "        if 4 in features:\n",
    "            return 1\n",
    "        elif -4 in features:\n",
    "            return -1\n",
    "        return 0\n",
    "\n",
    "    def play_single_game(self, game_num: int = 0) -> GameResult:\n",
    "        \"\"\"Play a single game between Alpha and Bravo\"\"\"\n",
    "        board = BoardProcessor()\n",
    "        moves = []\n",
    "\n",
    "        # Determine who plays first and assign player values\n",
    "        if self.config.alpha_plays_first:\n",
    "            alpha_player, bravo_player = 1, -1\n",
    "            current_is_alpha = True\n",
    "        else:\n",
    "            alpha_player, bravo_player = -1, 1\n",
    "            current_is_alpha = False\n",
    "\n",
    "        if self.config.verbose:\n",
    "            starter = \"Alpha\" if current_is_alpha else \"Bravo\"\n",
    "            print(f\"\\nGame {game_num + 1}: {starter} plays first\")\n",
    "\n",
    "        # Game loop\n",
    "        while True:\n",
    "            # Determine current player and model\n",
    "            if current_is_alpha:\n",
    "                player_value = alpha_player\n",
    "                model, scaler = self.model_alpha, self.scaler_alpha\n",
    "                epsilon = self.config.epsilon_alpha\n",
    "                player_name = \"Alpha\"\n",
    "            else:\n",
    "                player_value = bravo_player\n",
    "                model, scaler = self.model_bravo, self.scaler_bravo\n",
    "                epsilon = self.config.epsilon_bravo\n",
    "                player_name = \"Bravo\"\n",
    "\n",
    "            # Get move\n",
    "            col, q_val, q_values = self.get_ai_move(\n",
    "                board.state_list, player_value, epsilon, model, scaler\n",
    "            )\n",
    "\n",
    "            moves.append(col)\n",
    "            board.generate_state_list(moves)\n",
    "\n",
    "            if self.config.verbose:\n",
    "                print(f\"{player_name} plays column {col} (Q={q_val:.3f})\")\n",
    "\n",
    "            # Check for game end\n",
    "            winner = self.check_win(board.state_list)\n",
    "            if winner != 0:\n",
    "                # Convert winner to Alpha/Bravo perspective\n",
    "                if winner == alpha_player:\n",
    "                    result_winner = 1  # Alpha wins\n",
    "                    winner_name = \"Alpha\"\n",
    "                else:\n",
    "                    result_winner = -1  # Bravo wins\n",
    "                    winner_name = \"Bravo\"\n",
    "\n",
    "                if self.config.verbose:\n",
    "                    print(f\"{winner_name} wins in {len(moves)} moves! Code: {board.moves_code()}\")\n",
    "                break\n",
    "\n",
    "            if len(moves) >= 42:\n",
    "                result_winner = 0\n",
    "                if self.config.verbose:\n",
    "                    print(f\"Draw in {len(moves)} moves! Code: {board.moves_code()}\")\n",
    "                break\n",
    "\n",
    "            # Switch players\n",
    "            current_is_alpha = not current_is_alpha\n",
    "\n",
    "        return GameResult(\n",
    "            moves=moves,\n",
    "            winner=result_winner,\n",
    "            game_length=len(moves),\n",
    "            game_code=board.moves_code(),\n",
    "            alpha_first=self.config.alpha_plays_first,\n",
    "            epsilon_alpha=self.config.epsilon_alpha,\n",
    "            epsilon_bravo=self.config.epsilon_bravo\n",
    "        )\n",
    "\n",
    "    def run_tournament(self) -> Dict:\n",
    "        \"\"\"Run a tournament of multiple games\"\"\"\n",
    "        print(f\"\\n=== Starting tournament: {self.config.num_games} games ===\")\n",
    "        print(f\"Alpha ε={self.config.epsilon_alpha}, Bravo ε={self.config.epsilon_bravo}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for game_num in range(self.config.num_games):\n",
    "            result = self.play_single_game(game_num)\n",
    "\n",
    "            # Update statistics\n",
    "            if result.winner == 1:\n",
    "                self.stats['alpha_wins'] += 1\n",
    "            elif result.winner == -1:\n",
    "                self.stats['bravo_wins'] += 1\n",
    "            else:\n",
    "                self.stats['draws'] += 1\n",
    "\n",
    "            self.stats['total_games'] += 1\n",
    "            self.stats['game_results'].append(result)\n",
    "\n",
    "            # Alternate who plays first (optional)\n",
    "            if (game_num + 1) % 2 == 0:\n",
    "                self.config.alpha_plays_first = not self.config.alpha_plays_first\n",
    "\n",
    "        # Calculate final statistics\n",
    "        total_length = sum(r.game_length for r in self.stats['game_results'])\n",
    "        self.stats['avg_game_length'] = total_length / self.config.num_games\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # Print summary\n",
    "        self._print_tournament_summary(elapsed)\n",
    "\n",
    "        # Find and display most common game\n",
    "        self._display_most_common_game()\n",
    "\n",
    "        # Save results if requested\n",
    "        if self.config.save_games:\n",
    "            self._save_results()\n",
    "\n",
    "        return self.stats\n",
    "\n",
    "    def _print_tournament_summary(self, elapsed_time: float):\n",
    "        \"\"\"Print tournament results\"\"\"\n",
    "        print(f\"\\n=== Tournament Results ===\")\n",
    "        print(f\"Games played: {self.stats['total_games']}\")\n",
    "        print(f\"Alpha wins: {self.stats['alpha_wins']} ({self.stats['alpha_wins']/self.stats['total_games']*100:.1f}%)\")\n",
    "        print(f\"Bravo wins: {self.stats['bravo_wins']} ({self.stats['bravo_wins']/self.stats['total_games']*100:.1f}%)\")\n",
    "        print(f\"Draws: {self.stats['draws']} ({self.stats['draws']/self.stats['total_games']*100:.1f}%)\")\n",
    "        print(f\"Average game length: {self.stats['avg_game_length']:.1f} moves\")\n",
    "        print(f\"Time elapsed: {elapsed_time:.1f} seconds\")\n",
    "        print(f\"Games per second: {self.stats['total_games']/elapsed_time:.1f}\")\n",
    "\n",
    "    def _display_most_common_game(self):\n",
    "        \"\"\"Find and display the most common game pattern\"\"\"\n",
    "        if not self.stats['game_results']:\n",
    "            print(\"No games to analyze!\")\n",
    "            return\n",
    "\n",
    "        # Count game codes\n",
    "        from collections import Counter\n",
    "        game_codes = [result.game_code for result in self.stats['game_results']]\n",
    "        code_counts = Counter(game_codes)\n",
    "\n",
    "        if not code_counts:\n",
    "            print(\"No game codes found!\")\n",
    "            return\n",
    "\n",
    "        # Find most common\n",
    "        most_common_code, count = code_counts.most_common(1)[0]\n",
    "\n",
    "        print(f\"\\n=== MOST COMMON GAME PATTERN ===\")\n",
    "        print(f\"Game code: {most_common_code}\")\n",
    "        print(f\"Occurred {count} times out of {len(game_codes)} games ({count/len(game_codes)*100:.1f}%)\")\n",
    "\n",
    "        # Recreate and display the game\n",
    "        try:\n",
    "            board = BoardProcessor()\n",
    "            moves = board.decode_moves_code(most_common_code)\n",
    "            board.generate_state_list(moves)\n",
    "\n",
    "            print(f\"Move sequence: {moves}\")\n",
    "            print(f\"Game length: {len(moves)} moves\")\n",
    "            print(f\"Final board position:\")\n",
    "            board.display_board()\n",
    "\n",
    "            # Check winner\n",
    "            winner = self.check_win(board.state_list)\n",
    "            if winner == 1:\n",
    "                print(\"Winner: Player 1 (X)\")\n",
    "            elif winner == -1:\n",
    "                print(\"Winner: Player -1 (O)\")\n",
    "            else:\n",
    "                print(\"Result: Draw\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding game: {e}\")\n",
    "\n",
    "    def _save_results(self):\n",
    "        \"\"\"Save tournament results to file\"\"\"\n",
    "        save_data = {\n",
    "            'config': self.config,\n",
    "            'stats': self.stats,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "\n",
    "        with open(self.config.game_save_path, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "        print(f\"Results saved to {self.config.game_save_path}\")\n",
    "\n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot tournament results\"\"\"\n",
    "        if not self.stats['game_results']:\n",
    "            print(\"No games played yet!\")\n",
    "            return\n",
    "\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "        # Win percentages pie chart\n",
    "        labels = ['Alpha Wins', 'Bravo Wins', 'Draws']\n",
    "        sizes = [self.stats['alpha_wins'], self.stats['bravo_wins'], self.stats['draws']]\n",
    "        colors = ['lightblue', 'lightcoral', 'lightgray']\n",
    "        ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax1.set_title('Win Distribution')\n",
    "\n",
    "        # Game length histogram\n",
    "        lengths = [r.game_length for r in self.stats['game_results']]\n",
    "        ax2.hist(lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax2.set_xlabel('Game Length (moves)')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.set_title('Game Length Distribution')\n",
    "        ax2.axvline(np.mean(lengths), color='red', linestyle='--', label=f'Mean: {np.mean(lengths):.1f}')\n",
    "        ax2.legend()\n",
    "\n",
    "        # Running win rate\n",
    "        alpha_wins = []\n",
    "        running_alpha = 0\n",
    "        for i, result in enumerate(self.stats['game_results']):\n",
    "            if result.winner == 1:\n",
    "                running_alpha += 1\n",
    "            alpha_wins.append(running_alpha / (i + 1))\n",
    "\n",
    "        ax3.plot(alpha_wins, label='Alpha Win Rate', color='blue')\n",
    "        ax3.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax3.set_xlabel('Game Number')\n",
    "        ax3.set_ylabel('Win Rate')\n",
    "        ax3.set_title('Running Win Rate')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "\n",
    "        # Win rate by starting player\n",
    "        alpha_first_wins = sum(1 for r in self.stats['game_results']\n",
    "                              if r.alpha_first and r.winner == 1)\n",
    "        alpha_first_total = sum(1 for r in self.stats['game_results'] if r.alpha_first)\n",
    "\n",
    "        bravo_first_wins = sum(1 for r in self.stats['game_results']\n",
    "                              if not r.alpha_first and r.winner == -1)\n",
    "        bravo_first_total = sum(1 for r in self.stats['game_results'] if not r.alpha_first)\n",
    "\n",
    "        if alpha_first_total > 0 and bravo_first_total > 0:\n",
    "            first_player_advantage = [\n",
    "                alpha_first_wins / alpha_first_total,\n",
    "                bravo_first_wins / bravo_first_total\n",
    "            ]\n",
    "            ax4.bar(['Alpha plays first', 'Bravo plays first'], first_player_advantage,\n",
    "                   color=['lightblue', 'lightcoral'])\n",
    "            ax4.set_ylabel('Win Rate when playing first')\n",
    "            ax4.set_title('First Player Advantage')\n",
    "            ax4.set_ylim(0, 1)\n",
    "        else:\n",
    "            ax4.text(0.5, 0.5, 'Not enough alternating games', ha='center', va='center')\n",
    "            ax4.set_title('First Player Advantage (insufficient data)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "3cd3204018242dad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Example configurations for different experiments\n",
    "\n",
    "# Basic self-play with same model\n",
    "config_basic = SelfPlayConfig(\n",
    "    model_path_alpha=\"qnet_mc_pretrained.pth\",\n",
    "    model_path_bravo=\"qnet_mc_pretrained.pth\",\n",
    "    epsilon_alpha=0.1,\n",
    "    epsilon_bravo=0.1,\n",
    "    num_games=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Exploration vs Exploitation\n",
    "config_explore_exploit = SelfPlayConfig(\n",
    "    model_path_alpha=\"qnet_mc_pretrained.pth\",\n",
    "    model_path_bravo=\"qnet_mc_pretrained.pth\",\n",
    "    epsilon_alpha=0.3,  # High exploration\n",
    "    epsilon_bravo=0.05, # Low exploration\n",
    "    num_games=100,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Different models (when you have them)\n",
    "config_different_models = SelfPlayConfig(\n",
    "    model_path_alpha=\"qnet_mc_pretrained.pth\",\n",
    "    model_path_bravo=\"qnet_improved.pth\",  # When you create an improved model\n",
    "    epsilon_alpha=0.1,\n",
    "    epsilon_bravo=0.1,\n",
    "    num_games=200,\n",
    "    verbose=False\n",
    ")"
   ],
   "id": "ab324e0fe98f388f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Self-Play Arena ready!\")\n",
    "    print(\"Example usage:\")\n",
    "    print(\"arena = SelfPlayArena(config_basic)\")\n",
    "    print(\"results = arena.run_tournament()\")\n",
    "\n",
    "    # Uncomment to run a basic tournament\n",
    "    # arena = SelfPlayArena(config_basic)\n",
    "    # results = arena.run_tournament()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Quick test with 10 games\n",
    "config_basic.num_games = 1000\n",
    "config_basic.epsilon_alpha = 0.5\n",
    "config_basic.epsilon_bravo = 0.5\n",
    "arena = SelfPlayArena(config_basic)\n",
    "results = arena.run_tournament()\n",
    "arena.plot_results()"
   ],
   "id": "f16ab12045e217d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "def run_epsilon_sweep(arena, epsilon_values=[0.0, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0],\n",
    "                      games_per_epsilon=50, opponent_epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Run epsilon sweep using existing SelfPlayArena\n",
    "\n",
    "    Args:\n",
    "        arena: Initialized SelfPlayArena instance\n",
    "        epsilon_values: List of epsilon values to test\n",
    "        games_per_epsilon: Number of games per epsilon value\n",
    "        opponent_epsilon: Fixed epsilon for opponent\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with sweep results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    print(f\"Starting epsilon sweep: {len(epsilon_values)} values, {games_per_epsilon} games each\")\n",
    "    print(f\"Opponent epsilon fixed at {opponent_epsilon}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for eps in tqdm(epsilon_values, desc=\"Epsilon values\"):\n",
    "        # Configure arena for this epsilon\n",
    "        arena.config.epsilon_alpha = eps\n",
    "        arena.config.epsilon_bravo = opponent_epsilon\n",
    "        arena.config.num_games = games_per_epsilon\n",
    "        arena.config.verbose = False\n",
    "        arena.config.save_games = False\n",
    "\n",
    "        # Reset stats and run games\n",
    "        arena.reset_stats()\n",
    "        arena.run_tournament()\n",
    "\n",
    "        # Store results\n",
    "        results[eps] = {\n",
    "            'epsilon': eps,\n",
    "            'wins': arena.stats['alpha_wins'],\n",
    "            'losses': arena.stats['bravo_wins'],\n",
    "            'draws': arena.stats['draws'],\n",
    "            'win_rate': arena.stats['alpha_wins'] / games_per_epsilon,\n",
    "            'avg_game_length': arena.stats['avg_game_length'],\n",
    "            'games': arena.stats['game_results']\n",
    "        }\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        win_lengths = [g.game_length for g in arena.stats['game_results'] if g.winner == 1]\n",
    "        loss_lengths = [g.game_length for g in arena.stats['game_results'] if g.winner == -1]\n",
    "\n",
    "        results[eps]['avg_win_length'] = np.mean(win_lengths) if win_lengths else 0\n",
    "        results[eps]['avg_loss_length'] = np.mean(loss_lengths) if loss_lengths else 0\n",
    "\n",
    "        print(f\"ε={eps:.2f}: Win rate={results[eps]['win_rate']:.3f}, \"\n",
    "              f\"Avg game={results[eps]['avg_game_length']:.1f} moves\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_epsilon_sweep(results):\n",
    "    \"\"\"Create comprehensive visualization of epsilon sweep results\"\"\"\n",
    "\n",
    "    epsilons = sorted(results.keys())\n",
    "    metrics = {\n",
    "        'win_rate': [results[e]['win_rate'] for e in epsilons],\n",
    "        'draw_rate': [results[e]['draws']/len(results[e]['games']) for e in epsilons],\n",
    "        'avg_length': [results[e]['avg_game_length'] for e in epsilons],\n",
    "        'avg_win_length': [results[e]['avg_win_length'] for e in epsilons]\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Win rate vs epsilon\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epsilons, metrics['win_rate'], 'b-o', linewidth=2, markersize=8)\n",
    "    ax.fill_between(epsilons, metrics['win_rate'], alpha=0.3)\n",
    "    ax.set_xlabel('Epsilon (ε)', fontsize=12)\n",
    "    ax.set_ylabel('Win Rate', fontsize=12)\n",
    "    ax.set_title('Win Rate vs Exploration Rate', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "    # Optimal epsilon marker\n",
    "    best_eps = epsilons[np.argmax(metrics['win_rate'])]\n",
    "    best_rate = max(metrics['win_rate'])\n",
    "    ax.plot(best_eps, best_rate, 'r*', markersize=15, label=f'Best: ε={best_eps:.2f}')\n",
    "    ax.legend()\n",
    "\n",
    "    # Win/Loss/Draw distribution\n",
    "    ax = axes[0, 1]\n",
    "    width = 0.25\n",
    "    x = np.arange(len(epsilons))\n",
    "    wins = [results[e]['wins'] for e in epsilons]\n",
    "    losses = [results[e]['losses'] for e in epsilons]\n",
    "    draws = [results[e]['draws'] for e in epsilons]\n",
    "\n",
    "    ax.bar(x - width, wins, width, label='Wins', color='green', alpha=0.7)\n",
    "    ax.bar(x, losses, width, label='Losses', color='red', alpha=0.7)\n",
    "    ax.bar(x + width, draws, width, label='Draws', color='gray', alpha=0.7)\n",
    "    ax.set_xlabel('Epsilon (ε)', fontsize=12)\n",
    "    ax.set_ylabel('Number of Games', fontsize=12)\n",
    "    ax.set_title('Game Outcomes Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'{e:.2f}' for e in epsilons], rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Average game length\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(epsilons, metrics['avg_length'], 'g-s', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Epsilon (ε)', fontsize=12)\n",
    "    ax.set_ylabel('Average Game Length (moves)', fontsize=12)\n",
    "    ax.set_title('Game Length vs Exploration Rate', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Win rate heatmap with confidence\n",
    "    ax = axes[1, 1]\n",
    "    n_games = len(results[epsilons[0]]['games'])\n",
    "    conf_intervals = []\n",
    "    for e in epsilons:\n",
    "        wr = results[e]['win_rate']\n",
    "        # 95% confidence interval using normal approximation\n",
    "        ci = 1.96 * np.sqrt(wr * (1 - wr) / n_games)\n",
    "        conf_intervals.append(ci)\n",
    "\n",
    "    ax.errorbar(epsilons, metrics['win_rate'], yerr=conf_intervals,\n",
    "                fmt='o-', capsize=5, capthick=2, linewidth=2)\n",
    "    ax.fill_between(epsilons,\n",
    "                     [m - c for m, c in zip(metrics['win_rate'], conf_intervals)],\n",
    "                     [m + c for m, c in zip(metrics['win_rate'], conf_intervals)],\n",
    "                     alpha=0.2, label='95% CI')\n",
    "    ax.set_xlabel('Epsilon (ε)', fontsize=12)\n",
    "    ax.set_ylabel('Win Rate', fontsize=12)\n",
    "    ax.set_title('Win Rate with Confidence Intervals', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.suptitle(f'Epsilon Sweep Analysis ({n_games} games per ε)',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EPSILON SWEEP SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best epsilon: {best_eps:.2f} (win rate: {best_rate:.3f})\")\n",
    "    print(f\"Pure exploitation (ε=0.0): {results[0.0]['win_rate']:.3f}\")\n",
    "    print(f\"Pure exploration (ε=1.0): {results[1.0]['win_rate']:.3f}\")\n",
    "\n",
    "    # Find sweet spot range\n",
    "    threshold = best_rate - 0.05\n",
    "    good_epsilons = [e for e in epsilons if results[e]['win_rate'] >= threshold]\n",
    "    print(f\"Good epsilon range (within 5% of best): [{min(good_epsilons):.2f}, {max(good_epsilons):.2f}]\")\n",
    "\n",
    "    return best_eps\n",
    "\n",
    "\n",
    "def save_sweep_results(results, filename='epsilon_sweep_results.pkl'):\n",
    "    \"\"\"Save sweep results to file\"\"\"\n",
    "    save_data = {\n",
    "        'results': results,\n",
    "        'timestamp': time.time(),\n",
    "        'analysis': {\n",
    "            'best_epsilon': max(results.keys(), key=lambda e: results[e]['win_rate']),\n",
    "            'best_win_rate': max(r['win_rate'] for r in results.values())\n",
    "        }\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(save_data, f)\n",
    "    print(f\"\\nResults saved to {filename}\")\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Epsilon Sweep Analysis\")\n",
    "    print(\"Usage:\")\n",
    "    print(\"  from SPlay import SelfPlayArena, SelfPlayConfig\")\n",
    "    print(\"  config = SelfPlayConfig()\")\n",
    "    print(\"  arena = SelfPlayArena(config)\")\n",
    "    print(\"  results = run_epsilon_sweep(arena)\")\n",
    "    print(\"  plot_epsilon_sweep(results)\")\n",
    "    print(\"  save_sweep_results(results)\")"
   ],
   "id": "9b035e6524ca40a9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
